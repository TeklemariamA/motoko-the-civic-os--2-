﻿﻿# Copilot instructions for this repository

Goal: make an AI agent immediately productive by documenting how this ICP sample is stitched together (canisters, React UI, Ollama-backed LLM) and where the sharp edges are.

- **Canisters and layout**: [dfx.json](dfx.json) declares `backend` (type `motoko` but source is Python in [backend/app.mo](backend/app.mo)), `frontend` assets from [frontend/dist](frontend/dist) with entry [frontend/index.html](frontend/index.html), and remote `llm` canister id `w36hm-eqaaa-aaaal-qr76a-cai`. Output env goes to `.env`; packtool `mops sources`.
- **Backend reality check**: [backend/app.mo](backend/app.mo) is FastAPI-style Python, not Motoko. Endpoints: `/bounties/create`, `/bounties/{id}/current_value`, `/audit/private_action`, `/audit/public_log`, `/justice/file_case`, `/justice/cast_verdict`. All state is in-memory (users with salts/merit, bounties with time-based reward growth, audit log of proof hashes, merit-weighted juror selection). There is **no `chat` handler** here; the React UI will fail unless you add one or rewire the frontend.
- **Frontend flow**: [frontend/src/main.jsx](frontend/src/main.jsx) renders a chat UI that calls `backend.chat(messages)` from generated bindings. It stores history in localStorage key `civic_chat_history_v1`, shows a “Thinking ...” placeholder, strips it on response/error, and surfaces canister errors by parsing `SysTransient`/`CanisterReject`. Avatars live at `/bot.svg` and `/user.svg`; styling via [frontend/index.css](frontend/index.css) and Tailwind config in [frontend/tailwind.config.js](frontend/tailwind.config.js).
- **Bindings**: Generated at [frontend/src/declarations/backend/index.js](frontend/src/declarations/backend/index.js) (also mirrored under [src/declarations/backend/index.js](src/declarations/backend/index.js)). They are required for Vite to build; regenerate with `dfx generate backend` whenever the candid changes.
- **LLM dependency**: README expects a local Ollama server on port 11434; run `ollama serve` then `ollama run llama3.1:8b` once to download the model. The intended chat path is through the remote `llm` canister, but the missing backend `chat` means you must bridge the React UI to either the FastAPI endpoints or the LLM canister manually.
- **Dev workflow**: Root uses npm workspaces ([package.json](package.json)) pointing to `frontend`; run commands from the repo root (`npm install`, `npm run dev`, `npm run build`, `npm run prebuild`). `prebuild` in [frontend/package.json](frontend/package.json) installs dev deps then runs `dfx generate backend`. Typical local loop: `dfx start --background`, ensure bindings exist, then `npm run dev`. Clean deploy sample: `dfx start --background --clean && dfx deploy`; mainnet: `dfx deploy --network ic` after funding cycles (see [BUILD.md](BUILD.md)).
- **Devcontainer**: [./.devcontainer/devcontainer.json](.devcontainer/devcontainer.json) uses `ghcr.io/dfinity/icp-dev-env-slim:22`, forwards 4943 (dfx) and 5173 (vite), pre-installs the Motoko VS Code extension. Ports are forwarded; browser auto-opens for Vite.
- **State and persistence**: everything is ephemeral (in-memory backend, localStorage frontend). Restarting dfx or the server resets backend data; avoid assuming durable storage.
- **Conventions/gotchas**: Repo labeling suggests Motoko but backend is Python; don’t introduce Motoko-specific changes unless replacing the whole backend. Keep `llm` canister id/URLs in [dfx.json](dfx.json) in sync with any provider changes. If chat fails, verify bindings generation and that a `chat` endpoint exists or is stubbed.
- **What to add first when extending**: decide on a real chat path (FastAPI route, Motoko canister, or proxy to `llm` canister) and update both candid and [frontend/src/main.jsx](frontend/src/main.jsx) accordingly; add minimal tests or mocks if you introduce persistence or merit logic changes.

Ask for clarifications if any area feels thin (e.g., desired chat wiring or LLM routing).
